{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a612987",
   "metadata": {},
   "source": [
    "Q1. Explain One-Hot Encoding\n",
    "\n",
    "Ans:\n",
    "        A big part of the preprocessing is something encoding. This means representing each piece of data in a way that the computer can understand, hence the name encode, which literally means “convert to [computer] code”.\n",
    "        \n",
    "      **One-hot encoding is used to convert categorical variables into a numerical variable\n",
    "       format that can be used by machine learning algorithms.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f033a3f",
   "metadata": {},
   "source": [
    "Q2. Explain Bag of Words\n",
    "\n",
    "ans:\n",
    "\n",
    "Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. This approach is a simple and flexible way of extracting features from documents.\n",
    "\n",
    "A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order. It is called a “bag” of words because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "**A bag-of-words model constructs a statistical representation of text, by converting it into a matrix or grid with rows and columns.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07d028",
   "metadata": {},
   "source": [
    "Q3. Explain Bag of N-Grams\n",
    "\n",
    "ANS:\n",
    "\n",
    "An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6d352",
   "metadata": {},
   "source": [
    "Q4.Explain TF-IDF\n",
    "\n",
    "AnS:\n",
    "\n",
    "The scoring method being used above takes the count of each word and represents the word in the vector by the number of counts of that particular word.\n",
    "\n",
    "**tf(t,d) = count of t in d / number of words in d**\n",
    "\n",
    "**idf(t) = log(N/ df(t))**\n",
    "\n",
    "**tf-idf(t, d) = tf(t, d) * idf(t)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f68bd",
   "metadata": {},
   "source": [
    "Q5. What is OOV problem?\n",
    "\n",
    "**OOV is stand for Out OF Vocabulary**\n",
    "\n",
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5648533",
   "metadata": {},
   "source": [
    "Q6.What are word embeddings?\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "    \n",
    "    A very basic definition of a word embedding is a real number, vector representation of a word. Typically, these days, words with similar meaning will have vector representations that are close together in the embedding space (though this hasn’t always been the case).\n",
    "    \n",
    "    They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade9642",
   "metadata": {},
   "source": [
    "Q7. Explain Continuous bag of words (CBOW)\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    It attempts to guess the output (target word) from its neighboring words (context words)\n",
    "    \n",
    "    we will be implementing the CBOW for single-word architecture of Word2Vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc4f2f",
   "metadata": {},
   "source": [
    "Q8. Explain SkipGram\n",
    "\n",
    "Ans:\n",
    "    Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc3f97a",
   "metadata": {},
   "source": [
    "Q9. Explain Glove Embeddings.\n",
    "\n",
    "Ans:\n",
    "    GloVe stands for Global Vectors for word representation. It is an unsupervised learning algorithm developed by researchers at Stanford University aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
