{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e75ed7",
   "metadata": {},
   "source": [
    "1. Explain the architecture of BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that utilizes a multi-layer bidirectional Transformer encoder. The model is trained on a large corpus of text using masked language modeling and next sentence prediction tasks, allowing it to learn context-aware representations of words and sentences. The resulting model can be fine-tuned for a variety of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b72893",
   "metadata": {},
   "source": [
    "2. Explain Masked Language Modeling (MLM)\n",
    "\n",
    "Masked Language Modeling (MLM) is a task used in language model pretraining, where a small percentage of the tokens in a sentence are randomly masked and the model is trained to predict the original masked tokens. The objective is to learn the contextual representations of words, which can be used for a variety of downstream NLP tasks such as text classification, question answering, and machine translation. MLM is used in popular models like BERT, RoBERTa, and ALBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f74b5",
   "metadata": {},
   "source": [
    "3. Explain Next Sentence Prediction (NSP)\n",
    "\n",
    "Next Sentence Prediction (NSP) is a task used in pre-training of transformer-based models such as BERT, where the model learns to predict if two input sentences are consecutive or not. This helps the model learn the relationship between sentences and improves its ability to understand the context of a sentence in a larger text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b2eaa",
   "metadata": {},
   "source": [
    "4. What is Matthews evaluation?\n",
    "\n",
    "There is no standard evaluation method called \"Matthews evaluation\". However, the Matthews Correlation Coefficient (MCC) is a measure commonly used in machine learning to evaluate the performance of binary classification models. It takes into account true positives, true negatives, false positives, and false negatives to provide a balanced measure of the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59262723",
   "metadata": {},
   "source": [
    "5. What is Matthews Correlation Coefficient (MCC)?\n",
    "\n",
    "Matthews Correlation Coefficient (MCC) is a metric commonly used to evaluate the performance of binary classification models. It takes into account true and false positives and negatives and ranges between -1 and +1, where 1 indicates perfect correlation, 0 indicates no correlation, and -1 indicates total disagreement between predicted and actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b23899",
   "metadata": {},
   "source": [
    "6. Explain Semantic Role Labeling\n",
    "\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying the predicate-argument structure of a sentence, i.e., identifying the roles played by each entity in a sentence with respect to the predicate. SRL aims to identify semantic roles such as Agent, Patient, Instrument, etc., and has applications in question-answering systems, machine translation, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3ffa6",
   "metadata": {},
   "source": [
    "7. Why Fine-tuning a BERT model takes less time than pretraining\\\n",
    "\n",
    "Fine-tuning a BERT model takes less time than pretraining because the model has already learned general features and can be fine-tuned for specific tasks by adjusting only the task-specific parameters. This saves a lot of time and computational resources as the model doesn't need to learn everything from scratch during the fine-tuning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f64b7",
   "metadata": {},
   "source": [
    "8. Recognizing Textual Entailment (RTE)\n",
    "\n",
    "Recognizing Textual Entailment (RTE) is a task in natural language processing where the goal is to determine whether a given piece of text (hypothesis) can be inferred from another piece of text (premise). It is typically approached using machine learning techniques and is used in applications such as question answering and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33fd5c",
   "metadata": {},
   "source": [
    "9. Explain the decoder stack of GPT models.\n",
    "\n",
    "The decoder stack of GPT (Generative Pre-trained Transformer) models consists of a series of transformer decoder blocks. Each block includes multi-head self-attention and a feedforward network, followed by layer normalization and residual connections. These blocks are stacked on top of each other to form a deep neural network that generates text by predicting the next word in a sequence based on the preceding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c44f9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
