{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018c30b1",
   "metadata": {},
   "source": [
    "1. What are Vanilla autoencoders\n",
    "\n",
    "Vanilla autoencoders are neural networks designed to learn a compressed representation of input data by encoding the data into a lower-dimensional space and then decoding it back to its original form. They consist of an encoder and a decoder that are trained jointly to minimize the reconstruction error between the input and output. They are unsupervised learning models and can be used for various applications such as data compression, feature learning, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932845ea",
   "metadata": {},
   "source": [
    "2. What are Sparse autoencoders\n",
    "\n",
    "Sparse autoencoders are a type of neural network that learns to compress data by encoding it into a lower-dimensional representation, while preserving the essential features of the data. The sparsity constraint is used to encourage the network to learn a compressed representation that has fewer active neurons in the hidden layer. This can lead to better generalization and improved performance on tasks such as anomaly detection, data compression, and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e01709",
   "metadata": {},
   "source": [
    "3. What are Denoising autoencoders\n",
    "\n",
    "Denoising autoencoders are a type of neural network that is trained to remove noise from input data. It consists of two parts, an encoder that maps the noisy input to a latent representation, and a decoder that reconstructs the original input from the latent representation. Denoising autoencoders are trained to learn a robust representation of the input by reconstructing the original input from the noisy input, effectively learning to discard the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa0068",
   "metadata": {},
   "source": [
    "4. What are Convolutional autoencoders\n",
    "\n",
    "Convolutional autoencoders are a type of neural network architecture that uses convolutional layers for both encoding and decoding. They are commonly used in image compression and denoising tasks. The encoder takes an input image and compresses it into a lower-dimensional representation, while the decoder reconstructs the original image from the compressed representation. Convolutional autoencoders use convolutional layers to identify local features in the input image, making them more effective at capturing spatial relationships between pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d58e0",
   "metadata": {},
   "source": [
    "5. What are Stacked autoencoders\n",
    "\n",
    "Stacked autoencoders are a type of neural network architecture consisting of multiple layers of autoencoder models stacked on top of each other. Each layer of the stacked autoencoder is trained to learn increasingly complex features from the input data, with the final layer learning the most abstract representation of the data. This architecture can be used for tasks such as feature learning, unsupervised pre-training, and data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fc90d",
   "metadata": {},
   "source": [
    "6. Explain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "LSTM autoencoders can be used for generating sentences by first encoding a given sequence of words into a fixed-length vector representation using an encoder LSTM. Then, a decoder LSTM is used to generate a new sequence of words from the encoded vector. This process can be trained using a reconstruction loss, where the generated sentence is compared to the original sentence to minimize the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e036ab",
   "metadata": {},
   "source": [
    "7. Explain Extractive summarization\n",
    "\n",
    "Extractive summarization is a technique in natural language processing that involves selecting and combining sentences from a document to create a summary. It does not involve paraphrasing or generating new sentences, but instead relies on ranking the importance of each sentence based on its content and relevance to the overall document. Extractive summarization can be achieved through techniques such as clustering, graph-based methods, and machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afa946",
   "metadata": {},
   "source": [
    "8. Explain Abstractive summarization\n",
    "\n",
    "Abstractive summarization is a technique in natural language processing that involves generating a summary of a document by paraphrasing and condensing the key information rather than simply selecting and combining existing sentences. It involves understanding the content of the document and generating new, concise sentences that accurately capture its meaning. Abstractive summarization often uses deep learning models such as recurrent neural networks and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de514a6",
   "metadata": {},
   "source": [
    "9. Explain Beam search\n",
    "\n",
    "Beam search is a heuristic search algorithm used in natural language processing to generate a set of most likely output sequences from a given input sequence. It works by keeping track of a fixed number of the most promising partial output sequences, known as the beam width, at each time step and exploring all possible extensions of each of these sequences. The final output sequence is chosen based on the overall score of each of the generated sequences. Beam search is commonly used in tasks such as machine translation and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81f8c7",
   "metadata": {},
   "source": [
    "10. Explain Length normalization\n",
    "\n",
    "Length normalization is a technique used in natural language processing to address the problem of sentence length bias when comparing generated sentences of different lengths. It involves dividing the score of a generated sentence by a function of the sentence length, such as its number of words or characters, to give a fairer comparison between sentences of different lengths. This helps prevent shorter sentences from being favored over longer ones simply because they are shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33989e30",
   "metadata": {},
   "source": [
    "11. Explain Coverage normalization\n",
    "\n",
    "Coverage normalization is a method used in neural machine translation to address the issue of repeating or omitting words in the generated translations. It assigns a weight to each source word that indicates the extent to which it has been attended to in the previous time steps, and then normalizes the attention weights to prevent overemphasis on a few source words. This helps ensure that the generated translations cover all the important information in the source text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e43ead",
   "metadata": {},
   "source": [
    "12. Explain ROUGE metric evaluation\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics for evaluating the quality of summarization and text generation systems. It measures the overlap between a machine-generated summary or text and a set of human-written reference summaries or texts.\n",
    "\n",
    "There are several versions of the ROUGE metric, such as ROUGE-1, ROUGE-2, and ROUGE-L, each focusing on different aspects of overlap between the generated and reference texts. \n",
    "\n",
    "ROUGE-1 measures the overlap of unigrams (single words) between the generated and reference texts, while ROUGE-2 measures the overlap of bigrams (pairs of words) between the two. ROUGE-L measures the longest common subsequence between the generated and reference texts, rather than just the number of overlapping words.\n",
    "\n",
    "The output of ROUGE is a score between 0 and 1, with higher scores indicating a better match between the generated and reference texts.\n",
    "\n",
    "ROUGE has been widely used for evaluating the performance of summarization and text generation systems, especially in the field of natural language processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
